<!DOCTYPE html>
<html lang="en">

<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Dingkang Yang - Ph. D. Student - Fudan University</title>

    <link href="css/style.css" rel="stylesheet">
    <link href="css/header.css" rel="stylesheet">
    <link href="css/publications.css" rel="stylesheet">
    <link href="css/timeline.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Asap:400,400i,500" rel="stylesheet" type='text/css'>
    <link href="https://fonts.googleapis.com/css?family=Asap:400,400i,500" rel="stylesheet" type='text/css'>
    <link href="https://fonts.googleapis.com/css2?family=Space+Mono&display=swap" rel="stylesheet" type='text/css'>
    <style type="text/css"></style>

    <script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.0/jquery.min.js"></script>
    <script type="text/javascript">
        $(document).ready(function(){
            $(".trigger_abs").click(function(){ // trigger for abstract
                $(this).nextAll(".abstract").slideToggle("fast");
                $(this).children("a").toggleClass("closed open");
            });
            $(".trigger_bib").click(function(){ // trigger for bibtex
                $(this).nextAll(".bibtex").slideToggle("fast");
                $(this).children("a").toggleClass("closed open");
            });
        });
    </script>

</head>

<body>


<!-- HEADER -->
<div class="header">
    <div class="headermain">
        <div class="pubimg"><img src="img/dicken.jpg" class="profilepic"></div>
        <div style="display: table-cell; height: 120px; vertical-align: middle">
                <div class="name">Dingkang Yang</div>
                <div class="affiliation">Ph. D. Student at Fudan University, Shanghai</div>
                <div class="affiliation">Armed Forces Defence Student (Previously)</div>
                
        </div>
        <table>
            <tr>
                <th style="width: 50%; text-align: left"><img src="img/email.png" class="icon"> dkyang20[at]fudan.edu.cn</th>
                <th style="width: 50%; text-align: center"><a href="https://scholar.google.com/citations?user=jvlDhkcAAAAJ"><img src="img/scholar.png" class="icon"> Google Scholar</a></th>
                <!-- <th style="width: 33%; text-align: right"><a href="https://twitter.com/AlexRichardCS"><img src="img/twitter.png" class="icon"> Twitter</a></th> -->
            </tr>
        </table>
    </div>
</div>

<!-- CONTENT -->
<div class="main">
    <br>
    <br>
    I am a third-year Ph.D. student at Academy for Engineering and Technology of Fudan University. 
    As a part of the Intelligent Perception and Autonomous Systems Laboratory, I am advised by Prof. <a href="http://faet.fudan.edu.cn/e4/71/c23902a255089/page.htm" style="color: rgb(34, 113, 241);"> Lihua Zhang </a>.
    <br>
    <br>
    Before that, I received my B.S. degree under joint training from the School of Information, Yunnan University, and the <span style="color: rgb(34, 113, 241);">Chinese People's Armed Police Force</span>, in 2020. 
    <!-- My research interests are <b>vision-based fine-grained action recognition</b> and <b>action quality assessment</b> in medical scenes.
    I have also explored many vision tasks such as multi-target tracking, network quantization, human-object interaction detection and 6D pose estimation of satellites. -->
    
    <!-- PUBLICATIONS -->
    <a name="Interests">
        <div class="headline">Research Interests</div>
        <ul>
            <li>Multimodal Human Language Understanding</li>
            <br>
            <li>Sentiment Analysis / Emotion Recognition</li>
            <br>
            <li>Causal Inference/Unbiased Estimation</li>
            <br>
            <li>Multi-agent Collaborative Perception/V2X Communication in Autonomous Driving</li>
            <br>
            <li>AIGC/LLM</li>
            </ul>



    <!-- PUBLICATIONS -->
    <a name="publications">
    <div class="headline">Selected Publications</div>

    <!-- publication -->
    <div class="publication">
        <div class="pubimg"><img src="publications/img/CCIM.png" class="thumbnail"></div>
        <div class="pubtext">
            <div class="pubtitle">Context De-confounded Emotion Recognition</div>
            <div class="pubauthors"><b>Dingkang Yang</b>, Zhaoyu Chen, Yuzheng Wang, Shunli Wang, Mingcheng Li, ..., Lihua Zhang*</div>
            <div class="pubinfo">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR2023)</div>
            <div class="pubbox trigger_abs"><a href="javascript:void(0)">Abstract</a></div>
            <!-- <div class="pubbox"><a href="https://github.com/facebookresearch/multiface">Dataset</a></div> -->
            <div class="pubbox"><a href="publications/pdf/CCIM.pdf">PDF</a></div>
            <div class="pubbox"><a href=https://github.com/ydk122024/CCIM>Code</a></div>
            <div class="pubbox trigger_bib"><a href="javascript:void(0)">BibTex</a></div>
            <div class="abstract">Context-Aware Emotion Recognition (CAER) is a crucial and challenging task 
                that aims to perceive the emotional states of the target person with contextual information. 
                Recent approaches invariably focus on designing sophisticated architectures or mechanisms 
                to extract seemingly meaningful representations from subjects and contexts. However, a long-overlooked issue is that a context bias in existing datasets leads to a significantly unbalanced distribution of emotional states among different context scenarios. Concretely, the harmful bias is a confounder
                that misleads existing models to learn spurious correlations
                based on conventional likelihood estimation, significantly
                limiting the models' performance. To tackle the issue, this
                paper provides a causality-based perspective to disentangle the models from
                 the impact of such bias, and formulate the causalities among variables in the CAER task via
                a tailored causal graph. Then, we propose a Contextual
                Causal Intervention Module (CCIM) based on the backdoor
                adjustment to de-confound the confounder and exploit the
                true causal effect for model training. CCIM is plug-in and
                model-agnostic, which improves diverse state-of-the-art approaches by 
                considerable margins. Extensive experiments
                on three benchmark datasets demonstrate the effectiveness
                of our CCIM and the significance of causal insight.</div>
            <div class="bibtex"> @InProceedings{yang2023context,
            author = {Yang, Dingkang and Chen, Zhaoyu and Wang, Yuzheng and Wang, Shunli and Li, Mingcheng and Liu, Siao and Zhao, Xiao and Huang, Shuai and Dong, Zhiyan and Zhai, Peng and Zhang, Lihua},
            title = {Context De-Confounded Emotion Recognition},
            booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
            month  = {June},
            year = {2023},
            pages = {19005-19015}
            }
            </div>
        </div>
    </div>

    <!-- publication -->
    <div class="publication">
        <div class="pubimg"><img src="publications/img/KBS2023.png" class="thumbnail"></div>
        <div class="pubtext">
            <div class="pubtitle">Target and Source Modality Co-reinforcement for Emotion
                Understanding from Asynchronous Multimodal Sequences</div>
            <div class="pubauthors"><b>Dingkang Yang</b>, Yang Liu, Can Huang, ..., Peng Zhai*, Lihua Zhang*</div>
            <div class="pubinfo">Knowledge-Based Systems</div>
            <div class="pubbox trigger_abs"><a href="javascript:void(0)">Abstract</a></div>
            <!-- <div class="pubbox"><a href="https://github.com/facebookresearch/multiface">Dataset</a></div> -->
            <div class="pubbox"><a href="publications/pdf/KBS2023.pdf">PDF</a></div>
            <div class="pubbox trigger_bib"><a href="javascript:void(0)">BibTex</a></div>
            <div class="abstract">Perceiving human emotions from a multimodal perspective has received significant 
                attention in knowledge engineering communities. Due to the variable receiving frequency for sequences from various
                modalities, multimodal streams usually have an inherent asynchronous challenge. Most previous
                methods performed manual sequence alignment before multimodal fusion, which ignored long-range
                dependencies among modalities and failed to learn reliable crossmodal element correlations. Inspired
                by the human perception paradigm, we propose a target and source Modality Co-Reinforcement
                (MCR) approach to achieve sufficient crossmodal interaction and fusion at different granularities.
                Specifically, MCR introduces two types of target modality reinforcement units to reinforce the
                multimodal representations jointly. These target units effectively enhance emotion-related knowledge
                exchange in fine-grained interactions and capture the crossmodal elements that are emotionally
                expressive in mixed-grained interactions. Moreover, a source modality update module is presented to
                provide meaningful features for the crossmodal fusion of target modalities. Eventually, the multimodal
                representations are progressively reinforced and improved via the above components. Comprehensive
                experiments are conducted on three multimodal emotion understanding benchmarks. Quantitative
                results show that MCR significantly outperforms the previous state-of-the-art methods in both wordaligned 
                and unaligned settings. Additionally, qualitative analysis and visualization fully demonstrate
                the superiority of the proposed modules.</div>
            <div class="bibtex"> @article{yang2023target,
            title={Target and source modality co-reinforcement for emotion understanding from asynchronous multimodal sequences},
            author={Yang, Dingkang and Liu, Yang and Huang, Can and Li, Mingcheng and Zhao, Xiao and Wang, Yuzheng and Yang, Kun and Wang, Yan and Zhai, Peng and Zhang, Lihua},
            journal={Knowledge-Based Systems},
            volume={265},
            pages={110370},
            year={2023},
            publisher={Elsevier}
            }
            </div>
        </div>
    </div>

    <!-- publication -->
    <div class="publication">
        <div class="pubimg"><img src="publications/img/Avatar.png" class="thumbnail"></div>
        <div class="pubtext">
            <div class="pubtitle">Human 3D Avatar Modeling with Implicit Neural Representation:
                A Brief Survey</div>
            <div class="pubauthors">Mingyang Sun, <b>Dingkang Yang</b>, Dongliang Kou, Yang Jiang, Weihua Shan, Zhe Yan, Lihua Zhang*</div>
            <div class="pubinfo">A Brief Survey 2023</div>
            <div class="pubbox trigger_abs"><a href="javascript:void(0)">Abstract</a></div>
            <!-- <div class="pubbox"><a href="https://github.com/facebookresearch/multiface">Dataset</a></div> -->
            <div class="pubbox"><a href="publications/pdf/Avatar.pdf">PDF</a></div>
            <div class="pubbox trigger_bib"><a href="javascript:void(0)">BibTex</a></div>
            <div class="abstract">A human 3D avatar is one of the important elements in
                the metaverse, and the modeling effect directly affects people's visual experience. However, the human body has a
                complex topology and diverse details, so it is often expensive, time-consuming, and laborious to build a satisfactory
                model. Recent studies have proposed a novel method, implicit neural representation, which is a continuous representation method and can describe objects with arbitrary
                topology at arbitrary resolution. Researchers have applied
                implicit neural representation to human 3D avatar modeling and obtained more excellent results than traditional
                methods. This paper comprehensively reviews the application of implicit neural representation in human body modeling. First, we introduce three implicit representations of
                occupancy field, SDF, and NeRF, and make a classification
                of the literature investigated in this paper. Then the application of implicit modeling methods in the body, hand,
                and head are compared and analyzed respectively. Finally,
                we point out the shortcomings of current work and provide
                available suggestions for researchers.</div>
            <div class="bibtex">@article{sun2023human,
            title={Human 3D Avatar Modeling with Implicit Neural Representation: A Brief Survey},
            author={Sun, Mingyang and Yang, Dingkang and Kou, Dongliang and Jiang, Yang and Shan, Weihua and Yan, Zhe and Zhang, Lihua},
            journal={arXiv preprint arXiv:2306.03576},
            year={2023}
            }
            </div>
        </div>
    </div>


    <!-- publication -->
    <div class="publication">
        <div class="pubimg"><img src="publications/img/FDMER.png" class="thumbnail"></div>
        <div class="pubtext">
            <div class="pubtitle">Disentangled Representation Learning for Multimodal Emotion
                Recognition</div>
            <div class="pubauthors"><b>Dingkang Yang</b>, Shuai Huang, Haopeng Kuang, Yangtao Du, Lihua Zhang*</div>
            <div class="pubinfo">ACM International Conference on Multimedia (ACM-MM 2022)</div>
            <div class="pubbox trigger_abs"><a href="javascript:void(0)">Abstract</a></div>
            <!-- <div class="pubbox"><a href="https://github.com/facebookresearch/multiface">Dataset</a></div> -->
            <div class="pubbox"><a href="publications/pdf/FDMER.pdf">PDF</a></div>
            <div class="pubbox trigger_bib"><a href="javascript:void(0)">BibTex</a></div>
            <div class="abstract">Multimodal emotion recognition aims to identify human emotions from text, audio, and visual  modalities.
                Previous methods either explore correlations between different modalities or design sophisticated fusion strategies. However, the serious problem is that the distribution gap and information 
                redundancy often exist across heterogeneous modalities, resulting in learned multimodal representations that may be unrefined.
                Motivated by these observations, we propose a Feature-Disentangled Multimodal Emotion Recognition (FDMER) method, which learns the common and private feature representations for each modality.
                Specifically, we design the common and private encoders to project each modality into modality-invariant and modality-specific subspaces, respectively.
                The modality-invariant subspace aims to explore the commonality among different modalities and reduce the distribution gap sufficiently.
                The modality-specific subspaces attempt to enhance the diversity and capture the unique characteristics of each modality.
                After that, a modality discriminator is introduced to guide the parameter learning of the common and private encoders in an adversarial manner.
                We achieve the modality consistency and disparity constraints by designing tailored losses for the above subspaces.
                Furthermore, we present a cross-modal attention fusion module to learn adaptive weights for obtaining effective multimodal representations. The final representation is used for different downstream tasks.
                Experimental results show that the FDMER outperforms the state-of-the-art methods on two multimodal emotion recognition benchmarks. 
                Moreover, we further verify the effectiveness of our model via experiments on the multimodal humor detection task.</div>
            <div class="bibtex"> @inproceedings{yang2022disentangled,
            author = {Yang, Dingkang and Huang, Shuai and Kuang, Haopeng and Du, Yangtao and Zhang, Lihua},
            title = {Disentangled Representation Learning for Multimodal Emotion Recognition},
            booktitle={Proceedings of the 30th ACM International Conference on Multimedia (ACM MM)},
            year = {2022},
            pages = {1642-1651},
            numpages = {10}
            }
            </div>
        </div>
    </div>

    
   <!-- publication -->
   <div class="publication">
        <div class="pubimg"><img src="publications/img/MFSA.png" class="thumbnail"></div>
        <div class="pubtext">
            <div class="pubtitle">Learning Modality-Specific and -Agnostic Representations for Asynchronous Multimodal Language Sequences</div>
            <div class="pubauthors"><b>Dingkang Yang</b>,  Haopeng Kuang, Shuai Huang, Lihua Zhang*</div>
            <div class="pubinfo">ACM International Conference on Multimedia (ACM-MM 2022)</div>
            <div class="pubbox trigger_abs"><a href="javascript:void(0)">Abstract</a></div>
            <!-- <div class="pubbox"><a href="https://github.com/facebookresearch/multiface">Dataset</a></div> -->
            <div class="pubbox"><a href="publications/pdf/MFSA.pdf">PDF</a></div>
            <!-- <div class="pubbox"><a href="publications/pdf/MFSA_Supp.pdf">Supp</a></div> -->
            <div class="pubbox trigger_bib"><a href="javascript:void(0)">BibTex</a></div>
            <div class="abstract">Understanding human behaviors and intents from videos is a challenging task. Video flows usually involve time-series data from different modalities, such as natural language, facial gestures, and acoustic information.
                Due to the variable receiving frequency for sequences from each modality, the collected multimodal streams are usually unaligned.
                For multimodal fusion of asynchronous sequences, the existing methods focus on projecting multiple modalities into a common latent space and learning the hybrid representations, which neglects the diversity of each modality and the commonality across different modalities.
                Motivated by this observation, we propose a Multimodal Fusion approach for learning modality-Specific and modality-Agnostic representations (MFSA) to refine multimodal representations and leverage the complementarity across different modalities.
                Specifically, a predictive self-attention module is used to capture reliable contextual dependencies and enhance the unique features over the modality-specific spaces. 
                Meanwhile, we propose a hierarchical cross-modal attention module to explore the correlations between cross-modal elements over the modality-agnostic space.
                In this case, a double-discriminator strategy is presented to ensure the production of distinct representations in an adversarial manner.
                Eventually, the modality-specific and -agnostic multimodal representations are used together for downstream tasks.
                Comprehensive experiments on three multimodal datasets clearly demonstrate the superiority of our approach.</div>
            <div class="bibtex">@inproceedings{yang2022learning,
            author = {Yang, Dingkang and Kuang, Haopeng and Huang, Shuai and Zhang, Lihua},
            title = {Learning Modality-Specific and -Agnostic Representations for Asynchronous Multimodal Language Sequences},
            booktitle={Proceedings of the 30th ACM International Conference on Multimedia (ACM MM)},
            year = {2022},
            pages = {1708-1717},
            numpages = {10}
            } 
            </div>
        </div>
    </div>

    <!-- publication -->
    <div class="publication">
        <div class="pubimg"><img src="publications/img/ECCV.png" class="thumbnail"></div>
        <div class="pubtext">
            <div class="pubtitle">Emotion Recognition for Multiple Context Awareness</div>
            <div class="pubauthors"> <u><b>Dingkang Yang</b></u>, <u>Shuai Huang</u>, Shunli Wang, ..., Lihua Zhang*</div>
            <div class="pubinfo">European Conference on Computer Vision (ECCV 2022)</div>
            <div class="pubbox trigger_abs"><a href="javascript:void(0)">Abstract</a></div>
            <!-- <div class="pubbox"><a href="https://github.com/facebookresearch/multiface">Dataset</a></div> -->
            <div class="pubbox"><a href="publications/pdf/ECCV2022.pdf">PDF</a></div>
            <div class="pubbox"><a href="publications/pdf/ECCV_Supp.pdf">Supp</a></div>
            <div class="pubbox"><a href=https://heco2022.github.io>Project</a></div>
            <div class="pubbox"><a href=https://drive.google.com/drive/folders/1js1UoaAeagrHZ5eizInw5ZaWLJurKW-D>Data</a></div>
            <div class="pubbox trigger_bib"><a href="javascript:void(0)">BibTex</a></div>
            <div class="abstract">Understanding emotion in context is a rising hotspot in the
                computer vision community. Existing methods lack reliable context semantics to mitigate uncertainty in expressing emotions and fail to model
                multiple context representations complementarily. To alleviate these issues, we present a context-aware emotion recognition framework that
                combines four complementary contexts. The first context is multimodal
                emotion recognition based on facial expression, facial landmarks, gesture
                and gait. Secondly, we adopt the channel and spatial attention modules
                to obtain the emotion semantics of the scene context. Inspired by sociology theory, we explore the emotion transmission between agents by
                constructing relationship graphs in the third context. Meanwhile, we propose a novel agent-object context, which aggregates emotion cues from
                the interactions between surrounding agents and objects in the scene to
                mitigate the ambiguity of prediction. Finally, we introduce an adaptive
                relevance fusion module for learning the shared representations among
                multiple contexts. Extensive experiments show that our approach outperforms the state-of-the-art methods on both EMOTIC and GroupWalk
                datasets. We also release a dataset annotated with diverse emotion labels, Human Emotion in Context (HECO). In practice, we compare with
                the existing methods on the HECO, and our approach obtains a higher
                classification average precision of 50.65% and a lower regression mean
                error rate of 0.7. The project is available at <a href="https://heco2022.github.io/" style="color: blue;"> https://heco2022.github.io/ </a></div>
            <div class="bibtex">@inproceedings{yang2022emotion,
            author = {Dingkang Yang and
            Shuai Huang and
            Shunli Wang and
            Yang Liu and
            Peng Zhai and
            Liuzhen Su and
            Mingcheng Li and
            Lihua Zhang},
            title = {Emotion Recognition for Multiple Context Awareness},
            booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
            volume = {13697},
            pages = {144--162},
            year  = {2022}
            } </div>
        </div>
    </div>

   <!-- publication -->
   <div class="publication">
        <div class="pubimg"><img src="publications/img/SPL.png" class="thumbnail"></div>
        <div class="pubtext">
            <div class="pubtitle">Contextual and Cross-modal Interaction for Multi-modal Speech Emotion Recognition</div>
            <div class="pubauthors"><b>Dingkang Yang</b>, Shuai Huang, Yang Liu, Lihua Zhang*</div>
            <div class="pubinfo">IEEE SIGNAL PROCESSING LETTERS (SPL)</div>
            <div class="pubbox trigger_abs"><a href="javascript:void(0)">Abstract</a></div>
            <div class="pubbox"><a href="publications/pdf/SPL.pdf">PDF</a></div>
            <div class="pubbox trigger_bib"><a href="javascript:void(0)">BibTex</a></div>
            <div class="abstract">Speech emotion recognition combining linguistic
                content and audio signals in the dialog is a challenging task.
                Nevertheless, previous approaches have failed to explore emotion cues in contextual interactions and ignored the long-range
                dependencies between elements from different modalities. To
                tackle the above issues, this letter proposes a multimodal speech
                emotion recognition method using audio and text data. We first
                present a contextual transformer module to introduce contextual
                information via embedding the previous utterances between
                interlocutors, which enhances the emotion representation of the
                current utterance. Then, the proposed cross-modal transformer
                module focuses on the interactions between text and audio
                modalities, adaptively promoting the fusion from one modality
                to another. Furthermore, we construct associative topological
                relation over mini-batch and learn the association between deep
                fused features with graph convolutional network. Experimental
                results on the IEMOCAP and MELD datasets show that our
                method outperforms current state-of-the-art methods.</div>
            <div class="bibtex">@article{yang2022contextual,
            author={Yang, Dingkang and Huang, Shuai and Liu, Yang and Zhang, Lihua},
            journal={IEEE Signal Processing Letters},
            title={Contextual and Cross-Modal Interaction for Multi-Modal Speech Emotion Recognition},
            year={2022},
            volume={29},
            pages={2093-2097},
            }</div>
        </div>
    </div>



   <!-- publication -->
   <div class="publication">
        <div class="pubimg"><img src="publications/img/CA-SpaceNet.png" class="thumbnail"></div>
        <div class="pubtext">
            <div class="pubtitle">CA-SpaceNet: Counterfactual Analysis for 6D Pose Estimation in Space</div>
            <div class="pubauthors">Shunli Wang, Shuaibing Wang, Bo Jiao, <b>Dingkang Yang</b>, Liuzhen Su, Peng Zhai, Chixiao Chen, Lihua Zhang*</div>
            <div class="pubinfo">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2022)</div>
            <div class="pubbox trigger_abs"><a href="javascript:void(0)">Abstract</a></div>
            <div class="pubbox"><a href="publications/pdf/slwang_CA-SpaceNet.pdf">PDF</a></div>
            <div class="pubbox trigger_bib"><a href="javascript:void(0)">BibTex</a></div>
            <div class="abstract">Reliable and stable 6D pose estimation of uncooperative space objects plays an essential role in on-orbit servicing and debris removal 
                missions. Considering that the pose estimator is sensitive to background interference, this paper proposes a counterfactual analysis framework named CA-SpaceNet 
                to complete robust 6D pose estimation of the spaceborne targets under complicated background. Specifically, conventional methods are adopted to extract the 
                features of the whole image in the factual case. In the counterfactual case, a non-existent image without the target but only the background is imagined. 
                Side effect caused by background interference is reduced by counterfactual analysis, which leads to unbiased prediction in final results. In addition, 
                we also carry out lowbit-width quantization for CA-SpaceNet and deploy part of the framework to a Processing-In-Memory (PIM) accelerator on FPGA. 
                Qualitative and quantitative results demonstrate the effectiveness and efficiency of our proposed method. To our best knowledge, this paper applies 
                causal inference and network quantization to the 6D pose estimation of space-borne targets for the first time. 
                The code is available at <a href="https://github.com/Shunli-Wang/CA-SpaceNet" style="color: blue;"> https://github.com/Shunli-Wang/CA-SpaceNet </a>.</div>
            <div class="bibtex">@inproceedings{wang2022spacenet,
            title={CA-SpaceNet: Counterfactual Analysis for 6D Pose Estimation in Space},
            author={Wang, Shunli and Wang, Shuaibing and Jiao, Bo and Yang, Dingkang and Su, Liuzhen and Zhai, Peng and Chen, Chixiao and Zhang, Lihua},
            booktitle={2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
            pages={10627--10634},
            year={2022}
            } </div>
        </div>
    </div>





       <!-- publication -->
   <div class="publication">
        <div class="pubimg"><img src="publications/img/TSA-Net.png" class="thumbnail"></div>
        <div class="pubtext">
            <div class="pubtitle">TSA-Net: Tube Self-Attention Network for Action Quality Assessment</div>
            <div class="pubauthors">Shunli Wang, <b>Dingkang Yang</b>, Peng Zhai, Chixiao Chen, Lihua Zhang*</div>
            <div class="pubinfo">ACM International Conference on Multimedia (ACM-MM 2021)</div>
            <div class="pubbox trigger_abs"><a href="javascript:void(0)">Abstract</a></div>
            <div class="pubbox"><a href="publications/pdf/slwang_TSA-Net.pdf">PDF</a></div>
            <div class="pubbox"><a href="publications/pdf/slwang_TSA-Net_supp.pdf">Supp</a></div>
            <div class="pubbox"><a href=https://github.com/Shunli-Wang/TSA-Net>Code</a></div>
            <div class="pubbox trigger_bib"><a href="javascript:void(0)">BibTex</a></div>
            <div class="abstract">In recent years, assessing action quality from videos has attracted growing attention in computer vision community and human-computer interaction. 
                Most existing approaches usually tackle this problem by directly migrating the model from action recognition tasks, which ignores the intrinsic differences within the 
                feature map such as foreground and background information. To address this issue, we propose a Tube Self-Attention Network (TSA-Net) for action quality assessment (AQA). 
                Specifically, we introduce a single object tracker into AQA and propose the Tube Self-Attention Module (TSA), which can efficiently generate rich spatio-temporal contextual 
                information by adopting sparse feature interactions. The TSA module is embedded in existing video networks to form TSA-Net. Overall, our TSA-Net is with the following merits: 1) 
                High computational efficiency, 2) High flexibility, and 3) The state-of-the-art performance. Extensive experiments are conducted on popular action quality assessment datasets 
                including AQA-7 and MTL-AQA. Besides, a dataset named Fall Recognition in Figure Skating (FR-FS) is proposed to explore the basic action assessment in the figure skating scene. 
                Our TSA-Net achieves the Spearman's Rank Correlation of 0.8476 and 0.9393 on AQA-7 and MTL-AQA, respectively, which are the new state-of-the-art results. The results on FR-FS also 
                verify the effectiveness of the TSA-Net. The code and FR-FS dataset are publicly available at <a href="https://github.com/Shunli-Wang/TSA-Net" style="color: blue;"> https://github.com/Shunli-Wang/TSA-Net </a>.</div>
            <div class="bibtex"> @inproceedings{TSA-Net,
            title={TSA-Net: Tube Self-Attention Network for Action Quality Assessment},
            author={Wang, Shunli and Yang, Dingkang and Zhai, Peng and Chen, Chixiao and Zhang, Lihua},
            booktitle={Proceedings of the 29th ACM International Conference on Multimedia},
            year={2021},
            pages={4902-4910},
            numpages={9}
            }</div>
        </div>
    </div>

   <!-- publication -->
   <div class="publication">
        <div class="pubimg"><img src="publications/img/AQA.png" class="thumbnail"></div>
        <div class="pubtext">
            <div class="pubtitle">A Survey of Video-based Action Quality Assessment</div>
            <div class="pubauthors">Shunli Wang, <b>Dingkang Yang</b>, Peng Zhai, Qing Yu, Tao Suo, Zhan Sun, Ka Li, Lihua Zhang*</div>
            <div class="pubinfo">International Conference on Networking Systems of AI (INSAI 2021)</div>
            <div class="pubbox trigger_abs"><a href="javascript:void(0)">Abstract</a></div>
            <div class="pubbox"><a href="publications/pdf/slwang_AQA-Survey.pdf">PDF</a></div>
            <div class="pubbox trigger_bib"><a href="javascript:void(0)">BibTex</a></div>
            <div class="abstract">Human action recognition and analysis have great demand and important application significance in video surveillance, video retrieval, 
                and human-computer interaction. The task of human action quality evaluation requires the intelligent system to automatically and objectively evaluate the 
                action completed by the human. The action quality assessment model can reduce the human and material resources spent in action evaluation and reduce subjectivity. 
                In this paper, we provide a comprehensive survey of existing papers on video-based action quality assessment. Different from human action recognition, 
                the application scenario of action quality assessment is relatively narrow. Most of the existing work focuses on sports and medical care. We first introduce the 
                definition and challenges of human action quality assessment. Then we present the existing datasets and evaluation metrics. In addition, we summarized the methods 
                of sports and medical care according to the model categories and publishing institutions according to the characteristics of the two fields. At the end, combined 
                with recent work, the promising development direction in action quality assessment is discussed.</div>
            <div class="bibtex"> @inproceedings{wang2021survey,
                title={A Survey of Video-based Action Quality Assessment},
                author={Wang, Shunli and Yang, Dingkang and Zhai, Peng and Yu, Qing and Suo, Tao and Sun, Zhan and Li, Ka and Zhang, Lihua},
                booktitle={2021 International Conference on Networking Systems of AI (INSAI)},
                pages={1--9},
                year={2021}
              }
              </div>
        </div>
    </div>


   <!-- publication -->
   <div class="publication">
        <div class="pubimg"><img src="publications/img/ICIP.png" class="thumbnail"></div>
        <div class="pubtext">
            <div class="pubtitle">Learning Associative Representation For Facial Expression Recognition</div>
            <div class="pubauthors">Yangtao Du, <b>Dingkang Yang</b>, Peng Zhai, Mingcheng Li, Lihua Zhang*</div>
            <div class="pubinfo">IEEE International Conference on Image Processing (ICIP2021)</div>
            <div class="pubbox trigger_abs"><a href="javascript:void(0)">Abstract</a></div>
            <div class="pubbox"><a href="publications/pdf/ICIP.pdf">PDF</a></div>
            <div class="pubbox trigger_bib"><a href="javascript:void(0)">BibTex</a></div>
            <div class="abstract">The main inherent challenges with the Facial Expression
                Recognition (FER) are high intra-class variations and high
                inter-class similarities, while existing methods pay little
                attention to the association within inter- and intra-class
                expressions. This paper introduces a novel Expression
                Associative Network (EAN) to learn association of facial
                expression, specifically, from two aspects: 1) associative
                topological relation over mini-batch is constructed by
                similarity matrix with an adjacent regularization, and 2)
                learning association of expressions with Graph
                Convolutional Network (GCN). Besides, an auxiliary
                module as invariant feature generator based on Generative
                Adversarial Networks (GAN) is designed to suppress pose
                variations, illumination changes, and occlusions. Results on
                public benchmarks achieve comparable or better
                performance compared with current state-of-the-art
                methods, with 90.07% on FERPlus, 86.36% on RAF-DB,
                and improve by 3.92% over SOTA on synthetic wrong
                labeling datasets.</div>
            <div class="bibtex"> @inproceedings{du2021learning,
            title={Learning Associative Representation For Facial Expression Recognition},
            author={Du, Yangtao and Yang, Dingkang and Zhai, Peng and Li, Mingchen and Zhang, Lihua},
            booktitle={2021 IEEE International Conference on Image Processing (ICIP)},
            pages={889-893},
            year={2021}
            }</div>
        </div>
    </div>



    <!-- EDUCATION -->
    <a name="patents">
    <div class="headline"> Invention Patents</div>

    <!-- publication -->
    <div class="publication">
        <div class="pubimg"><img src="publications/img/z1.png" class="thumbnail"></div>
        <div class="pubtext">
            <div class="pubtitle">Multimodal Data Annotation Device and Programmable Computer
                Readable Storage Mediums</div>
            <div class="pubauthors">Inventors: Lihua Zhang, <b>Dingkang Yang</b>, Peixuan Zhang, Peng Zhai</div>
            <div class="pubauthors">Patentee: Fudan University</div>
            <div class="pubinfo">Publication Number (Granted): CN111881979B</div>
            <div class="pubbox"><a href="publications/pdf/Multimodal Data Annotation Device.pdf">PDF</a></div>
            <div class="abstract">    </div>
        </div>
    </div>

    <!-- publication -->
    <div class="publication">
        <div class="pubimg"><img src="publications/img/z1.png" class="thumbnail"></div>
        <div class="pubtext">
            <div class="pubtitle">A Deep Learning-based Multimodal Perception and Analysis System for Patient Behaviour</div>
            <div class="pubauthors">Inventors: Lihua Zhang, <b>Dingkang Yang</b>, Peng Zhai, Zhiyan Dong</div>
            <div class="pubauthors">Patentee: Fudan University</div>
            <div class="pubinfo">Publication Number (Granted): CN111914925B</div>
            <div class="pubbox"><a href="publications/pdf/A_deep_learning-based_multimodal_perception.pdf">PDF</a></div>
            <div class="abstract">    </div>
        </div>
    </div>

    <!-- publication -->
    <div class="publication">
        <div class="pubimg"><img src="publications/img/z1.png" class="thumbnail"></div>
        <div class="pubtext">
            <div class="pubtitle">  A Statistical Learning-based Multimodal Analysis and Prediction System for Patient Behaviour</div>
            <div class="pubauthors">Inventors: Lihua Zhang, <b>Dingkang Yang</b>, Haopeng Kuang, Lin Ye</div>
            <div class="pubauthors">Patentee: Fudan University</div>
            <div class="pubinfo">Publication Number (Initiative for Examination as to Substance): CN111920420A</div>
            <div class="pubbox"><a href="publications/pdf/A_Statistical _Learning-based _Multimodal _Analysis.pdf">PDF</a></div>
            <div class="abstract">    </div>
        </div>
    </div>


    <!-- publication -->
    <div class="publication">
        <div class="pubimg"><img src="publications/img/z1.png" class="thumbnail"></div>
        <div class="pubtext">
            <div class="pubtitle">  A Context-aware Multimodal Emotion Recognition Approach and System</div>
            <div class="pubauthors">Inventors: Lihua Zhang, <b>Dingkang Yang</b>, Shunli Wang, Haopeng Kuang, Shuai Huang</div>
            <div class="pubauthors">Patentee: Fudan University</div>
            <div class="pubinfo">Publication Number (Initiative for Examination as to Substance): CN113947702A</div>
            <div class="pubbox"><a href="publications/pdf/A_context-aware_multimodal_emotion_recognition.pdf">PDF</a></div>
            <div class="abstract">    </div>
        </div>
    </div>


    <!-- publication -->
    <div class="publication">
        <div class="pubimg"><img src="publications/img/z1.png" class="thumbnail"></div>
        <div class="pubtext">
            <div class="pubtitle">A Multimodal Emotion Recognition Approach and System for Companion Robots</div>
            <div class="pubauthors">Inventors: Lihua Zhang, Shuai Huang, <b>Dingkang Yang</b>, Shunli Wang, Haopeng Kuang</div>
            <div class="pubauthors">Patentee: Fudan University</div>
            <div class="pubinfo">Publication Number (Initiative for Examination as to Substance): CN113947127A</div>
            <div class="pubbox"><a href="publications/pdf/A_multimodal_emotion_recognition_approach_and_system_for_companion_robots.pdf">PDF</a></div>
            <div class="abstract">    </div>
        </div>
    </div>


    <!-- publication -->
    <div class="publication">
        <div class="pubimg"><img src="publications/img/z1.png" class="thumbnail"></div>
        <div class="pubtext">
            <div class="pubtitle">A Deep Learning-based Quality Assessment Approach for Athlete Behaviour</div>
            <div class="pubauthors">Inventors: Lihua Zhang, Shunli Wang, <b>Dingkang Yang</b>, Haopeng Kuang</div>
            <div class="pubauthors">Patentee: Fudan University</div>
            <div class="pubinfo">Publication Number (Initiative for Examination as to Substance): CN113989920A</div>
            <div class="pubbox"><a href="publications/pdf/A_Deep_Learning-based_Quality_Assessment_Approach_for_Athlete_Behaviour.pdf">PDF</a></div>
            <div class="abstract">    </div>
        </div>
    </div>


    <!-- publication -->
    <div class="publication">
        <div class="pubimg"><img src="publications/img/z1.png" class="thumbnail"></div>
        <div class="pubtext">
            <div class="pubtitle">A Dynamic 3D Reconstruction Approach and System for Hepatectomy</div>
            <div class="pubauthors">Inventors: Lihua Zhang, Haopeng Kuang, <b>Dingkang Yang</b>, Shunli Wang, Zhongwei Yang</div>
            <div class="pubauthors">Patentee: Fudan University</div>
            <div class="pubinfo">Publication Number (Initiative for Examination as to Substance): CN113888698A</div>
            <div class="pubbox"><a href="publications/pdf/A_Dynamic_3D_Reconstruction_Approach_and_System_for_Hepatectomy.pdf">PDF</a></div>
            <div class="abstract">    </div>
        </div>
    </div>

   
   
  

    <!-- EDUCATION -->
    <a name="education">
    <div class="headline">Education/Work</div>

    <!-- timeline -->
    <div class="timeline">
        <!-- entry -->
        <div class="entry">
            <div class="title">
                <p class="textbf">Sep 2020 - Current</p>
                <p>Ph. D. Student at Academy for Engineering and Technology, Fudan University, Shanghai<p>
            </div>
        <div class="body">
            <p>Research Topics: Multimodal Learning, Sentiment Analysis, Autonomous Driving, and Causal Inference.</p>
        </div>
        <!-- entry -->
        <div class="entry">
            <div class="title">
                <p class="textbf">Sep 2016 - Jun 2020</p>
                <p>Bachelor's Degree, Yunnan University<p>
            </div>
        <div class="body">
            <p>Thesis: Mobile Robotic Fire Extinguishing System in a Tobacco Factory Warehouse Context.</p>
        </div>
    </div>

    


</div>

</body>
