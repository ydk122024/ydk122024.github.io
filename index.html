<!DOCTYPE html>
<html lang="en">

<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Dingkang Yang - Ph. D. Student - Fudan University</title>

    <link href="css/style.css" rel="stylesheet">
    <link href="css/header.css" rel="stylesheet">
    <link href="css/publications.css" rel="stylesheet">
    <link href="css/timeline.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Asap:400,400i,500" rel="stylesheet" type='text/css'>
    <link href="https://fonts.googleapis.com/css?family=Asap:400,400i,500" rel="stylesheet" type='text/css'>
    <link href="https://fonts.googleapis.com/css2?family=Space+Mono&display=swap" rel="stylesheet" type='text/css'>
    <style type="text/css"></style>

    <script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.0/jquery.min.js"></script>
    <script type="text/javascript">
        $(document).ready(function(){
            $(".trigger_abs").click(function(){ // trigger for abstract
                $(this).nextAll(".abstract").slideToggle("fast");
                $(this).children("a").toggleClass("closed open");
            });
            $(".trigger_bib").click(function(){ // trigger for bibtex
                $(this).nextAll(".bibtex").slideToggle("fast");
                $(this).children("a").toggleClass("closed open");
            });
        });
    </script>

</head>

<body>


<!-- HEADER -->
<div class="header">
    <div class="headermain">
        <div class="pubimg"><img src="img/dicken.jpg" class="profilepic"></div>
        <div style="display: table-cell; height: 120px; vertical-align: middle">
                <div class="name">Dingkang Yang</div>
                <div class="affiliation">Ph. D. Student at Fudan University, Shanghai</div>
                <div class="affiliation">Armed Forces Defence Student (Previously)</div>
                
        </div>
        <table>
            <tr>
                <th style="width: 50%; text-align: left"><img src="img/email.png" class="icon"> dkyang20@fudan.edu.cn</th>
                <th style="width: 50%; text-align: center"><a href="https://scholar.google.com/citations?user=jvlDhkcAAAAJ"><img src="img/scholar.png" class="icon"> Google Scholar</a></th>
                <!-- <th style="width: 33%; text-align: right"><a href="https://twitter.com/AlexRichardCS"><img src="img/twitter.png" class="icon"> Twitter</a></th> -->
            </tr>
        </table>
    </div>
</div>

<!-- CONTENT -->
<div class="main">
    <br>
    <br>
    I am a third-year Ph.D. student at Academy for Engineering and Technology of Fudan University. 
    As a part of the Intelligent Perception and Autonomous Systems Laboratory, I am advised by Prof. <a href="http://faet.fudan.edu.cn/e4/71/c23902a255089/page.htm" style="color: brown;"> Lihua Zhang </a>.
    Before that, I received my B.S. degree under joint training from the School of Information, Yunnan University, and the Chinese People's Armed Police Force (Optional Training Office), in 2020.
    <!-- My research interests are <b>vision-based fine-grained action recognition</b> and <b>action quality assessment</b> in medical scenes.
    I have also explored many vision tasks such as multi-target tracking, network quantization, human-object interaction detection and 6D pose estimation of satellites. -->
    


    <!-- PUBLICATIONS -->
    <a name="publications">
    <div class="headline">Publications</div>

    <!-- publication -->
    <div class="publication">
        <div class="pubimg"><img src="publications/img/FDMER.png" class="thumbnail"></div>
        <div class="pubtext">
            <div class="pubtitle">Disentangled Representation Learning for Multimodal Emotion
                Recognition</div>
            <div class="pubauthors"><b>Dingkang Yang</b>, Shuai Huang, Haopeng Kuang, Yangtao Du, Lihua Zhang*</div>
            <div class="pubinfo">ACM International Conference on Multimedia (ACM-MM 2022)</div>
            <div class="pubbox trigger_abs"><a href="javascript:void(0)">Abstract</a></div>
            <!-- <div class="pubbox"><a href="https://github.com/facebookresearch/multiface">Dataset</a></div> -->
            <div class="pubbox"><a href="publications/pdf/FDMER.pdf">PDF</a></div>
            <div class="pubbox trigger_bib"><a href="javascript:void(0)">BibTex</a></div>
            <div class="abstract">Multimodal emotion recognition aims to identify human emotions from text, audio, and visual  modalities.
                Previous methods either explore correlations between different modalities or design sophisticated fusion strategies. However, the serious problem is that the distribution gap and information 
                redundancy often exist across heterogeneous modalities, resulting in learned multimodal representations that may be unrefined.
                Motivated by these observations, we propose a Feature-Disentangled Multimodal Emotion Recognition (FDMER) method, which learns the common and private feature representations for each modality.
                Specifically, we design the common and private encoders to project each modality into modality-invariant and modality-specific subspaces, respectively.
                The modality-invariant subspace aims to explore the commonality among different modalities and reduce the distribution gap sufficiently.
                The modality-specific subspaces attempt to enhance the diversity and capture the unique characteristics of each modality.
                After that, a modality discriminator is introduced to guide the parameter learning of the common and private encoders in an adversarial manner.
                We achieve the modality consistency and disparity constraints by designing tailored losses for the above subspaces.
                Furthermore, we present a cross-modal attention fusion module to learn adaptive weights for obtaining effective multimodal representations. The final representation is used for different downstream tasks.
                Experimental results show that the FDMER outperforms the state-of-the-art methods on two multimodal emotion recognition benchmarks. 
                Moreover, we further verify the effectiveness of our model via experiments on the multimodal humor detection task.</div>
            <div class="bibtex"> </div>
        </div>
    </div>

   <!-- publication -->
   <div class="publication">
        <div class="pubimg"><img src="publications/img/MFSA.png" class="thumbnail"></div>
        <div class="pubtext">
            <div class="pubtitle">Learning Modality-Specific and -Agnostic Representations for Asynchronous Multimodal Language Sequences</div>
            <div class="pubauthors"><b>Dingkang Yang</b>,  Haopeng Kuang, Shuai Huang, Lihua Zhang*</div>
            <div class="pubinfo">ACM International Conference on Multimedia (ACM-MM 2022)</div>
            <div class="pubbox trigger_abs"><a href="javascript:void(0)">Abstract</a></div>
            <!-- <div class="pubbox"><a href="https://github.com/facebookresearch/multiface">Dataset</a></div> -->
            <div class="pubbox"><a href="publications/pdf/MFSA.pdf">PDF</a></div>
            <div class="pubbox"><a href="publications/pdf/MFSA_Supp.pdf">Supp</a></div>
            <div class="pubbox trigger_bib"><a href="javascript:void(0)">BibTex</a></div>
            <div class="abstract">Understanding human behaviors and intents from videos is a challenging task. Video flows usually involve time-series data from different modalities, such as natural language, facial gestures, and acoustic information.
                Due to the variable receiving frequency for sequences from each modality, the collected multimodal streams are usually unaligned.
                For multimodal fusion of asynchronous sequences, the existing methods focus on projecting multiple modalities into a common latent space and learning the hybrid representations, which neglects the diversity of each modality and the commonality across different modalities.
                Motivated by this observation, we propose a Multimodal Fusion approach for learning modality-Specific and modality-Agnostic representations (MFSA) to refine multimodal representations and leverage the complementarity across different modalities.
                Specifically, a predictive self-attention module is used to capture reliable contextual dependencies and enhance the unique features over the modality-specific spaces. 
                Meanwhile, we propose a hierarchical cross-modal attention module to explore the correlations between cross-modal elements over the modality-agnostic space.
                In this case, a double-discriminator strategy is presented to ensure the production of distinct representations in an adversarial manner.
                Eventually, the modality-specific and -agnostic multimodal representations are used together for downstream tasks.
                Comprehensive experiments on three multimodal datasets clearly demonstrate the superiority of our approach.</div>
            <div class="bibtex"> </div>
        </div>
    </div>

    <!-- publication -->
    <div class="publication">
        <div class="pubimg"><img src="publications/img/ECCV.png" class="thumbnail"></div>
        <div class="pubtext">
            <div class="pubtitle">Emotion Recognition for Multiple Context Awareness</div>
            <div class="pubauthors"> <u><b>Dingkang Yang</b></u>, <u>Shuai Huang</u>, Shunli Wang, ..., Lihua Zhang*</div>
            <div class="pubinfo">European Conference on Computer Vision (ECCV 2022)</div>
            <div class="pubbox trigger_abs"><a href="javascript:void(0)">Abstract</a></div>
            <!-- <div class="pubbox"><a href="https://github.com/facebookresearch/multiface">Dataset</a></div> -->
            <div class="pubbox"><a href="publications/pdf/ECCV.pdf">PDF</a></div>
            <div class="pubbox"><a href="publications/pdf/ECCV_Supp.pdf">Supp</a></div>
            <div class="pubbox"><a href=https://heco2022.github.io>Project</a></div>
            <div class="pubbox"><a href=https://drive.google.com/drive/folders/1js1UoaAeagrHZ5eizInw5ZaWLJurKW-D>Data</a></div>
            <div class="pubbox trigger_bib"><a href="javascript:void(0)">BibTex</a></div>
            <div class="abstract">Understanding emotion in context is a rising hotspot in the
                computer vision community. Existing methods lack reliable context semantics to mitigate uncertainty in expressing emotions and fail to model
                multiple context representations complementarily. To alleviate these issues, we present a context-aware emotion recognition framework that
                combines four complementary contexts. The first context is multimodal
                emotion recognition based on facial expression, facial landmarks, gesture
                and gait. Secondly, we adopt the channel and spatial attention modules
                to obtain the emotion semantics of the scene context. Inspired by sociology theory, we explore the emotion transmission between agents by
                constructing relationship graphs in the third context. Meanwhile, we propose a novel agent-object context, which aggregates emotion cues from
                the interactions between surrounding agents and objects in the scene to
                mitigate the ambiguity of prediction. Finally, we introduce an adaptive
                relevance fusion module for learning the shared representations among
                multiple contexts. Extensive experiments show that our approach outperforms the state-of-the-art methods on both EMOTIC and GroupWalk
                datasets. We also release a dataset annotated with diverse emotion labels, Human Emotion in Context (HECO). In practice, we compare with
                the existing methods on the HECO, and our approach obtains a higher
                classification average precision of 50.65% and a lower regression mean
                error rate of 0.7. The project is available at <a href="https://heco2022.github.io/" style="color: blue;"> https://heco2022.github.io/ </a></div>
            <div class="bibtex"> </div>
        </div>
    </div>

   <!-- publication -->
   <div class="publication">
        <div class="pubimg"><img src="publications/img/SPL.png" class="thumbnail"></div>
        <div class="pubtext">
            <div class="pubtitle">Contextual and Cross-modal Interaction for Multi-modal Speech Emotion Recognition</div>
            <div class="pubauthors"><b>Dingkang Yang</b>, Shuai Huang, Yang Liu, Xiao Zhao, Siao Liu, Lihua Zhang*</div>
            <div class="pubinfo">IEEE SIGNAL PROCESSING LETTERS (SPL)</div>
            <div class="pubbox trigger_abs"><a href="javascript:void(0)">Abstract</a></div>
            <div class="pubbox"><a href="publications/pdf/SPL.pdf">PDF</a></div>
            <div class="pubbox trigger_bib"><a href="javascript:void(0)">BibTex</a></div>
            <div class="abstract">Speech emotion recognition combining linguistic
                content and audio signals in the dialog is a challenging task.
                Nevertheless, previous approaches have failed to explore emotion cues in contextual interactions and ignored the long-range
                dependencies between elements from different modalities. To
                tackle the above issues, this letter proposes a multimodal speech
                emotion recognition method using audio and text data. We first
                present a contextual transformer module to introduce contextual
                information via embedding the previous utterances between
                interlocutors, which enhances the emotion representation of the
                current utterance. Then, the proposed cross-modal transformer
                module focuses on the interactions between text and audio
                modalities, adaptively promoting the fusion from one modality
                to another. Furthermore, we construct associative topological
                relation over mini-batch and learn the association between deep
                fused features with graph convolutional network. Experimental
                results on the IEMOCAP and MELD datasets show that our
                method outperforms current state-of-the-art methods.</div>
            <div class="bibtex"> </div>
        </div>
    </div>



   <!-- publication -->
   <div class="publication">
        <div class="pubimg"><img src="publications/img/CA-SpaceNet.png" class="thumbnail"></div>
        <div class="pubtext">
            <div class="pubtitle">CA-SpaceNet: Counterfactual Analysis for 6D Pose Estimation in Space</div>
            <div class="pubauthors">Shunli Wang, Shuaibing Wang, Bo Jiao, <b>Dingkang Yang</b>, Liuzhen Su, Peng Zhai, Chixiao Chen, Lihua Zhang*</div>
            <div class="pubinfo">2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2022)</div>
            <div class="pubbox trigger_abs"><a href="javascript:void(0)">Abstract</a></div>
            <div class="pubbox"><a href="publications/pdf/slwang_CA-SpaceNet.pdf">PDF</a></div>
            <div class="pubbox trigger_bib"><a href="javascript:void(0)">BibTex</a></div>
            <div class="abstract">Reliable and stable 6D pose estimation of uncooperative space objects plays an essential role in on-orbit servicing and debris removal 
                missions. Considering that the pose estimator is sensitive to background interference, this paper proposes a counterfactual analysis framework named CA-SpaceNet 
                to complete robust 6D pose estimation of the spaceborne targets under complicated background. Specifically, conventional methods are adopted to extract the 
                features of the whole image in the factual case. In the counterfactual case, a non-existent image without the target but only the background is imagined. 
                Side effect caused by background interference is reduced by counterfactual analysis, which leads to unbiased prediction in final results. In addition, 
                we also carry out lowbit-width quantization for CA-SpaceNet and deploy part of the framework to a Processing-In-Memory (PIM) accelerator on FPGA. 
                Qualitative and quantitative results demonstrate the effectiveness and efficiency of our proposed method. To our best knowledge, this paper applies 
                causal inference and network quantization to the 6D pose estimation of space-borne targets for the first time. 
                The code is available at <a href="https://github.com/Shunli-Wang/CA-SpaceNet" style="color: blue;"> https://github.com/Shunli-Wang/CA-SpaceNet </a> .</div>
            <div class="bibtex"> </div>
        </div>
    </div>


   <!-- publication -->
   <div class="publication">
        <div class="pubimg"><img src="publications/img/VAD.png" class="thumbnail"></div>
        <div class="pubtext">
            <div class="pubtitle">Learning Appearance-motion Normality for Video Anomaly Detection</div>
            <div class="pubauthors">Yang Liu, Jing Liu, Mengyang Zhao, <b>Dingkang Yang</b>, ...</div>
            <div class="pubinfo">IEEE International Conference on Multimedia and Expo (ICME 2022)</div>
            <div class="pubbox trigger_abs"><a href="javascript:void(0)">Abstract</a></div>
            <div class="pubbox"><a href="publications/pdf/VAD.pdf">PDF</a></div>
            <div class="pubbox trigger_bib"><a href="javascript:void(0)">BibTex</a></div>
            <div class="abstract">Video anomaly detection is a challenging task in the computer vision community. Most single task-based methods do
                not consider the independence of unique spatial and temporal
                patterns, while two-stream structures lack the exploration of
                the correlations. In this paper, we propose spatial-temporal
                memories augmented two-stream auto-encoder framework,
                which learns the appearance normality and motion normality independently and explores the correlations via adversarial learning. Specifically, we first design two proxy tasks to
                train the two-stream structure to extract appearance and motion features in isolation. Then, the prototypical features are
                recorded in the corresponding spatial and temporal memory
                pools. Finally, the encoding-decoding network performs adversarial learning with the discriminator to explore the correlations between spatial and temporal patterns. Experimental
                results show that our framework outperforms the state-of-theart methods, achieving AUCs of 98.1% and 89.8% on UCSD
                Ped2 and CUHK Avenue datasets.</div>
            <div class="bibtex">@inproceedings{liu2022learning,
                title={Learning Appearance-Motion Normality for Video Anomaly Detection},
                author={Liu, Yang and Liu, Jing and Zhao, Mengyang and Yang, Dingkang and Zhu, Xiaoguang and Song, Liang},
                booktitle={2022 IEEE International Conference on Multimedia and Expo (ICME)},
                pages={1--6},
                year={2022},
                organization={IEEE Computer Society}
              } </div>
        </div>
    </div>



       <!-- publication -->
   <div class="publication">
        <div class="pubimg"><img src="publications/img/TSA-Net.png" class="thumbnail"></div>
        <div class="pubtext">
            <div class="pubtitle">TSA-Net: Tube Self-Attention Network for Action Quality Assessment</div>
            <div class="pubauthors">Shunli Wang, <b>Dingkang Yang</b>, Peng Zhai, Chixiao Chen, Lihua Zhang*</div>
            <div class="pubinfo">ACM International Conference on Multimedia (ACM-MM 2021)</div>
            <div class="pubbox trigger_abs"><a href="javascript:void(0)">Abstract</a></div>
            <div class="pubbox"><a href="publications/pdf/slwang_TSA-Net.pdf">PDF</a></div>
            <div class="pubbox"><a href="publications/pdf/slwang_TSA-Net_supp.pdf">Supp</a></div>
            <div class="pubbox"><a href=https://github.com/Shunli-Wang/TSA-Net>Project</a></div>
            <div class="pubbox trigger_bib"><a href="javascript:void(0)">BibTex</a></div>
            <div class="abstract">In recent years, assessing action quality from videos has attracted growing attention in computer vision community and human-computer interaction. 
                Most existing approaches usually tackle this problem by directly migrating the model from action recognition tasks, which ignores the intrinsic differences within the 
                feature map such as foreground and background information. To address this issue, we propose a Tube Self-Attention Network (TSA-Net) for action quality assessment (AQA). 
                Specifically, we introduce a single object tracker into AQA and propose the Tube Self-Attention Module (TSA), which can efficiently generate rich spatio-temporal contextual 
                information by adopting sparse feature interactions. The TSA module is embedded in existing video networks to form TSA-Net. Overall, our TSA-Net is with the following merits: 1) 
                High computational efficiency, 2) High flexibility, and 3) The state-of-the-art performance. Extensive experiments are conducted on popular action quality assessment datasets 
                including AQA-7 and MTL-AQA. Besides, a dataset named Fall Recognition in Figure Skating (FR-FS) is proposed to explore the basic action assessment in the figure skating scene. 
                Our TSA-Net achieves the Spearman's Rank Correlation of 0.8476 and 0.9393 on AQA-7 and MTL-AQA, respectively, which are the new state-of-the-art results. The results on FR-FS also 
                verify the effectiveness of the TSA-Net. The code and FR-FS dataset are publicly available at <a href="https://github.com/Shunli-Wang/TSA-Net" style="color: blue;"> https://github.com/Shunli-Wang/TSA-Net </a>.</div>
            <div class="bibtex"> @inproceedings{TSA-Net,
                title={TSA-Net: Tube Self-Attention Network for Action Quality Assessment},
                author={Wang, Shunli and Yang, Dingkang and Zhai, Peng and Chen, Chixiao and Zhang, Lihua},
                booktitle={Proceedings of the 29th ACM International Conference on Multimedia},
                year={2021},
                pages={4902–4910},
                numpages={9}
              }</div>
        </div>
    </div>

   <!-- publication -->
   <div class="publication">
        <div class="pubimg"><img src="publications/img/AQA.png" class="thumbnail"></div>
        <div class="pubtext">
            <div class="pubtitle">A Survey of Video-based Action Quality Assessment</div>
            <div class="pubauthors">Shunli Wang, <b>Dingkang Yang</b>, Peng Zhai, Qing Yu, Tao Suo, Zhan Sun, Ka Li, Lihua Zhang*</div>
            <div class="pubinfo">International Conference on Networking Systems of AI (INSAI 2021)</div>
            <div class="pubbox trigger_abs"><a href="javascript:void(0)">Abstract</a></div>
            <div class="pubbox"><a href="publications/pdf/slwang_AQA-Survey.pdf">PDF</a></div>
            <div class="pubbox trigger_bib"><a href="javascript:void(0)">BibTex</a></div>
            <div class="abstract">Human action recognition and analysis have great demand and important application significance in video surveillance, video retrieval, 
                and human-computer interaction. The task of human action quality evaluation requires the intelligent system to automatically and objectively evaluate the 
                action completed by the human. The action quality assessment model can reduce the human and material resources spent in action evaluation and reduce subjectivity. 
                In this paper, we provide a comprehensive survey of existing papers on video-based action quality assessment. Different from human action recognition, 
                the application scenario of action quality assessment is relatively narrow. Most of the existing work focuses on sports and medical care. We first introduce the 
                definition and challenges of human action quality assessment. Then we present the existing datasets and evaluation metrics. In addition, we summarized the methods 
                of sports and medical care according to the model categories and publishing institutions according to the characteristics of the two fields. At the end, combined 
                with recent work, the promising development direction in action quality assessment is discussed.</div>
            <div class="bibtex"> @inproceedings{AQA_Survey,
                title={A Survey of Video-based Action Quality Assessment}, 
                author={Wang, Shunli and Yang, Dingkang and Zhai, Peng and Yu, Qing and Suo, Tao and Sun, Zhan and Li, Ka and Zhang, Lihua},
                booktitle={2021 International Conference on Networking Systems of AI (INSAI)}, 
                year={2021},
                volume={},
                number={},
                pages={1-9},
                doi={10.1109/INSAI54028.2021.00029}
              }</div>
        </div>
    </div>

   <!-- publication -->
   <div class="publication">
        <div class="pubimg"><img src="publications/img/MAPP.png" class="thumbnail"></div>
        <div class="pubtext">
            <div class="pubtitle">An Improved Ant Colony Optimization Algorithm for Multi-Agent Path Planning</div>
            <div class="pubauthors">Shuai Huang, <b>Dingkang Yang</b>, Chuyi Zhong, Shichao Yan, Lihua Zhang*</div>
            <div class="pubinfo">International Conference on Networking Systems of AI (INSAI 2021)</div>
            <div class="pubbox trigger_abs"><a href="javascript:void(0)">Abstract</a></div>
            <div class="pubbox"><a href="publications/pdf/MAPP.pdf">PDF</a></div>
            <div class="pubbox trigger_bib"><a href="javascript:void(0)">BibTex</a></div>
            <div class="abstract">In recent years, Ant Colony Optimization
                algorithm has become one of the most widely used heuristic
                algorithms and has been apply to solve different types of path
                planning problems. However, there still are some problems in
                Multi-Agent Path Finding, such as low convergence efficiency,
                easy to fall into local optimum and vertex conflict. In this
                paper, we proposed an Improved Ant Colony Optimization
                algorithm based on parameter optimization and vertex conflict
                resolution. First of all, we initialize the distribution of
                pheromones to reduce the blindness of the algorithm in the
                early stage. Secondly, we introduce an adaptive pheromone
                intensity and pheromone reduction factor to avoid the
                algorithm falling into local optimum. On this basis, the
                algorithmÿs global search ability and convergence speed are
                improved by dynamic modification of the evaporation factor
                and heuristic function. In addition, the strategy of dynamically
                modifying the influence factor and heuristic function improves
                the global search ability and convergence speed of the
                algorithm. To solve vertex conflict in MAPF, we use the design
                conflict prediction and resolution strategy to effectively avoid
                vertex conflict and improve the reliability of the multi-agent
                system. Simulation experiments verify the effectiveness and
                adaptability of IACO under different complexity environments,
                and prove that IACO has good convergence speed and path
                global optimization ability.</div>
            <div class="bibtex"> @inproceedings{huang2021improved,
                title={An Improved Ant Colony Optimization Algorithm for Multi-Agent Path Planning},
                author={Huang, Shuai and Yang, Dingkang and Zhong, Chuyi and Yan, Shichao and Zhang, Lihua},
                booktitle={2021 International Conference on Networking Systems of AI (INSAI)},
                pages={95--100},
                year={2021},
                organization={IEEE}
              }</div>
        </div>
    </div>


   <!-- publication -->
   <div class="publication">
        <div class="pubimg"><img src="publications/img/ICIP.png" class="thumbnail"></div>
        <div class="pubtext">
            <div class="pubtitle">Learning Associative Representation For Facial Expression Recognition</div>
            <div class="pubauthors">Yangtao Du, <b>Dingkang Yang</b>, Peng Zhai, Mingcheng Li, Lihua Zhang*</div>
            <div class="pubinfo">2021 IEEE International Conference on Image Processing (ICIP2021)</div>
            <div class="pubbox trigger_abs"><a href="javascript:void(0)">Abstract</a></div>
            <div class="pubbox"><a href="publications/pdf/ICIP.pdf">PDF</a></div>
            <div class="pubbox trigger_bib"><a href="javascript:void(0)">BibTex</a></div>
            <div class="abstract">The main inherent challenges with the Facial Expression
                Recognition (FER) are high intra-class variations and high
                inter-class similarities, while existing methods pay little
                attention to the association within inter- and intra-class
                expressions. This paper introduces a novel Expression
                Associative Network (EAN) to learn association of facial
                expression, specifically, from two aspects: 1) associative
                topological relation over mini-batch is constructed by
                similarity matrix with an adjacent regularization, and 2)
                learning association of expressions with Graph
                Convolutional Network (GCN). Besides, an auxiliary
                module as invariant feature generator based on Generative
                Adversarial Networks (GAN) is designed to suppress pose
                variations, illumination changes, and occlusions. Results on
                public benchmarks achieve comparable or better
                performance compared with current state-of-the-art
                methods, with 90.07% on FERPlus, 86.36% on RAF-DB,
                and improve by 3.92% over SOTA on synthetic wrong
                labeling datasets.</div>
            <div class="bibtex"> @inproceedings{du2021learning,
                title={Learning Associative Representation For Facial Expression Recognition},
                author={Du, Yangtao and Yang, Dingkang and Zhai, Peng and Li, Mingchen and Zhang, Lihua},
                booktitle={2021 IEEE International Conference on Image Processing (ICIP)},
                pages={889--893},
                year={2021},
                organization={IEEE}
              }</div>
        </div>
    </div>


    <!-- EDUCATION -->
    <a name="education">
    <div class="headline">Education/Work</div>

    <!-- timeline -->
    <div class="timeline">
        <!-- entry -->
        <div class="entry">
            <div class="title">
                <p class="textbf">Sep 2020 - current</p>
                <p>Ph. D. Student at Academy for Engineering and Technology, Fudan University, Shanghai<p>
            </div>
        <div class="body">
            <p>Research Topics: Multimodal Learning, Sentiment Analysis and Causal Inference</p>
        </div>
        <!-- entry -->
        <div class="entry">
            <div class="title">
                <p class="textbf">Sep 2016 - Jun 2020</p>
                <p>Bachelor's Degree, Yunnan University<p>
            </div>
        <div class="body">
            <p>Thesis: Mobile Robotic Fire Extinguishing System in a Tobacco Factory Warehouse Context.</p>
        </div>
    </div>


</div>

</body>
